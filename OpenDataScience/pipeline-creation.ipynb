{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15bece81e965d431d04fa854f13a5c521a2c4f2b"
   },
   "source": [
    " ## Creating basic Pipelines using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5d096ecf0c04316c4ccd5e131a29adb51bfca4a"
   },
   "source": [
    "> ### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import eli5\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display_html\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "061cd96f2871c6572f7b4baf45b65bb3cb0bba2a"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../input/'\n",
    "SEED = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af1ef1a33dd2e3d88681f094c6ddf56b159b376e"
   },
   "source": [
    "> ### Reading Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "592eef7b187bc2c102ea1201122e8d55c289c59f"
   },
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv')\n",
    "path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv')\n",
    "path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl')\n",
    "train_df = pd.read_csv(path_to_train,\n",
    "                   index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv(path_to_test,\n",
    "                  index_col='session_id', parse_dates=times)\n",
    "\n",
    "with open(path_to_site_dict, 'rb') as f:\n",
    "    site2id = pickle.load(f)\n",
    "# create an inverse id _> site mapping\n",
    "id2site = {v:k for (k, v) in site2id.items()}\n",
    "# we treat site with id 0 as \"unknown\"\n",
    "id2site[0] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1fda17ee1e909739cb22f6de45e6bc05bf368f9"
   },
   "source": [
    "> ### Custom Debugger to print intermediate values\n",
    "\n",
    "### Usage\n",
    "```\n",
    "                    Pipeline( [\n",
    "                    ('check',FunctionTransformer(randomfunction, validate=False)),              \n",
    "                    (\"debug1\", Debug())] )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c32f0eabae6cd20cc2579a155c4697f60b891453"
   },
   "outputs": [],
   "source": [
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"Degugger Start\")\n",
    "        print(X[1:5])\n",
    "        # what other output you want\n",
    "        print(\"End\")\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1288a2593f9feb518729c73d59cbe40dd550fa83"
   },
   "source": [
    "> ### Pipeline Specific Functions.\n",
    "\n",
    "If the transformer is not taking any additional input, transformer function can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "11a966213fc4b53449fd4058cb1f1441f7dcafae"
   },
   "outputs": [],
   "source": [
    "def concatfunction(data):\n",
    "    return data[sites].fillna(0).astype('int').apply(lambda row: \n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64500994c77868aaea21b0a433d7d276c15a9107"
   },
   "source": [
    ">  Custom Transformer\n",
    "\n",
    "For passing additional parameters, custom transformer is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a20b9ef92bd1bd838d0119f5b115edd86a187c62"
   },
   "outputs": [],
   "source": [
    "class add_time_features(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract time features from datetime column\"\"\"\n",
    "\n",
    "    def __init__(self,column='time1',add_hour=False):\n",
    "        self.column=column\n",
    "        self.add_hour=add_hour\n",
    "\n",
    "    def transform(self, data, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        times = ['time%s' % i for i in range(1, 11)]\n",
    "        times=data[times]\n",
    "        hour = times[self.column].apply(lambda ts: ts.hour)\n",
    "        morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n",
    "        day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n",
    "        evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n",
    "        night = ((hour >= 0) & (hour <=6)).astype('int').values.reshape(-1, 1)\n",
    "        objects_to_hstack = [ morning, day, evening, night]\n",
    "        feature_names = ['morning', 'day', 'evening', 'night']\n",
    "        if self.add_hour:\n",
    "        # scale hour dividing by 24\n",
    "            objects_to_hstack.append(hour.values.reshape(-1, 1) / 24)\n",
    "            feature_names.append('hour')\n",
    "        return pd.DataFrame(np.hstack(objects_to_hstack),columns=feature_names,index=data.index)\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "08cba5beeded4cd78c76195350208247298ab7c6"
   },
   "outputs": [],
   "source": [
    "vectorizer_params={'ngram_range': (1, 5), \n",
    "                   'max_features': 100000,\n",
    "                   'tokenizer': lambda s: s.split()}\n",
    "time_split = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9279fbb2df59683de68a095e4a281f5f07de13d"
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "07b6beb9dcda5045d434c97cae66dc62448b96cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# data --+-->concatenate sites in a session-->tf-idf vectorizer--+-->FeatureUnion-->Logistic Regression\n",
    "#        |                                                       |\n",
    "#        +--> extracting time features from start date column  --+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85bea556d709eb4ac15445b01add35c91d4303cc"
   },
   "source": [
    "1. FeatureUnion: http://michelleful.github.io/code-blog/2015/06/20/pipelines/\n",
    "2. FunctionTransformer: https://stackoverflow.com/questions/39001956/sklearn-pipeline-how-to-apply-different-transformations-on-different-columns/39009125#39009125        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "38db0b2276e062f65978c66b9ecf8a3e53f0fa3e"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pipeline = Pipeline( [\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "                    ('tf-idf features',\n",
    "                     Pipeline([  \n",
    "                            ('concatenate sites',FunctionTransformer(concatfunction, validate=False)),  \n",
    "                            ('vectorizing text',TfidfVectorizer(**vectorizer_params))\n",
    "                             ])\n",
    "                    )\n",
    "                    ,\n",
    "                    ('time features',\n",
    "                     Pipeline([\n",
    "                             ('time1 features',add_time_features(column='time1',add_hour=False))\n",
    "                            ])\n",
    "                    )\n",
    "                            ]\n",
    "                            )\n",
    "        ),\n",
    "        ('classifier',LogisticRegression(C=1, random_state=SEED, solver='liblinear'))\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de32fbf4eb7041bbca80bb31e787ea0c6ea0e44a"
   },
   "source": [
    "> ### Fitting Pipeline on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4337b121ca5e901181ac474649d666ba5c8c6666"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('tf-idf features', Pipeline(memory=None,\n",
       "     steps=[('concatenate sites', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function concatfunction at 0x7f17304f3048>,\n",
       "          inv_kw_args=None, inverse...alty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_df.drop(columns='target'),train_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23777d55b885e92c9177433fac43d6c287c8eb1c"
   },
   "source": [
    "> ### Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "97172347bac7458acbe7772ca5d75926e4453624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
